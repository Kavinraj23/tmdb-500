# -*- coding: utf-8 -*-
"""01_data_preprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16NdwSQrbLoS263hH0KYbo9CKkcVcBb_9
"""






























# -*- coding: utf-8 -*-
"""02_feature_engineering.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sBzhxoboGmoifNqiEyE1K2okAt1XvWpY
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
df_cleaned = pd.read_csv('final_dataset.csv')

import seaborn as sns
import matplotlib.pyplot as plt

df_copy = df_cleaned.copy()
df_copy.drop(columns=["movie_id", "title_x", "title_y"], inplace=True)

corr_matrix_cleaned = df_copy.select_dtypes(include=["float64", "int64"]).corr()

plt.figure(figsize=(18, 14))
sns.heatmap(corr_matrix_cleaned, cmap='coolwarm', center=0, linewidths=0.5)
plt.title("Correlation Matrix (Cleaned Feature Set)", fontsize=16)
plt.show()

target_corr = corr_matrix_cleaned["log_revenue"].drop("log_revenue").sort_values(ascending=False)

print("Top 10 positively correlated features with log_revenue:")
print(target_corr.head(10))

print("\nTop 10 negatively correlated features with log_revenue:")
print(target_corr.tail(10))

"""Since vote ratio has a very negative correlation, they add noise to the dataset, we will drop it from the final dataset."""

target_corr['runtime_bucket']

df_cleaned.drop(columns=["vote_ratio"], inplace=True)

"""To reduce dimensionality and help models generalize better, we are grouping together low-impact features into 'genre_other'."""

low_corr_genres = ['Romance', 'Western', 'Music', 'Horror', 'Drama', 'TV Movie', 'Documentary']
df_cleaned['genre_other'] = df_cleaned[low_corr_genres].max(axis=1)
df_cleaned.drop(columns=low_corr_genres, inplace=True)

df.to_csv('final_dataset_v2.csv', index=False)

# -*- coding: utf-8 -*-
"""03_modeling_and_evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JyOoHjYGSjvoXn_C40dUhnkfYo_P99by
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

df = pd.read_csv('final_dataset_v2.csv')
df = df[df['log_revenue'] > 0] # reflect chagne in 02
# drop runtime_bucket in 02
X = df.drop(columns=['log_revenue', 'title_x', 'title_y', 'movie_id', 'runtime_bucket'], errors='ignore')
y = df['log_revenue']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

def evaluate_model(y_true, y_pred, name="Model"):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)
    print(f"\n{name} Performance:")
    print(f"MAE:  {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"R^2:  {r2:.2f}")
    return mae, rmse, r2
def train_and_evaluate(model, X_train, y_train, X_test, y_test, name="Model"):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    return evaluate_model(y_test, y_pred, name)

"""Baseline Model"""

y_pred_baseline = np.full_like(y_test, y_train.mean())
evaluate_model(y_test, y_pred_baseline, "Baseline")

"""Random Forest"""

rf_model = RandomForestRegressor(random_state=42)
train_and_evaluate(rf_model, X_train, y_train, X_test, y_test, "Random Forest")

"""XGBoost"""

xgb_default = XGBRegressor(random_state=42)
train_and_evaluate(xgb_default, X_train, y_train, X_test, y_test, "XGBoost Default")

"""Cross-Validation for XGBoost"""

cv_scores = cross_val_score(xgb_default, X, y, cv=5, scoring='neg_mean_absolute_error')
print("\nXGBoost Cross-Validated MAE:", -cv_scores.mean())

"""Hyperparameter Tuning"""

param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0],
    'reg_alpha': [0, 0.1, 1],
    'reg_lambda': [0.5, 1, 2]
}
tuner = RandomizedSearchCV(
    XGBRegressor(random_state=42),
    param_distributions=param_grid,
    scoring='neg_mean_absolute_error',
    n_iter=30,
    cv=3,
    verbose=1,
    n_jobs=-1
)
tuner.fit(X_train, y_train)
print("\nBest Parameters:", tuner.best_params_)

"""XGboost Model with Tuned Parameters"""

xgb_tuned = tuner.best_estimator_
train_and_evaluate(xgb_tuned, X_train, y_train, X_test, y_test, "XGBoost Tuned")

importances = xgb_tuned.feature_importances_
feature_importance_df = pd.DataFrame({
    'feature': X.columns,
    'importance': importances
}).sort_values(by='importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(data=feature_importance_df.head(15), x='importance', y='feature')
plt.title("Top 15 Feature Importances (XGBoost)")
plt.tight_layout()
plt.show()

"""Dropping leaky features from the full dataset

Testing feature interactions
"""

X_interact = X.copy()
X_interact['budget_popularity'] = X_interact['log_budget'] * X_interact['popularity']
X_interact['budget_vote_ratio'] = X_interact['log_budget'] * X_interact['vote_ratio']


# Split
X_train, X_test, y_train, y_test = train_test_split(X_interact, y, test_size=0.2, random_state=42)

# Train model
xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)
evaluate_model(y_test, y_pred, name="XGBoost with Feature Interactions")

X_interact['runtime_budget'] = X_interact['runtime'] * X_interact['log_budget']
X_interact['budget_year'] = X_interact['log_budget'] * X_interact['release_year']
X_train_i, X_test_i, y_train_i, y_test_i = train_test_split(X_interact, y, test_size=0.2, random_state=42)
xgb_model = XGBRegressor(random_state=42)
xgb_model.fit(X_train_i, y_train_i)
y_pred_i = xgb_model.predict(X_test_i)
evaluate_model(y_test_i, y_pred_i, name="XGB with Extra Interactions")

param_grid = {
    'n_estimators': [200, 300],
    'max_depth': [5, 7],
    'learning_rate': [0.05, 0.1],
}

grid_search = GridSearchCV(
    XGBRegressor(random_state=42),
    param_grid,
    scoring='neg_mean_absolute_error',
    cv=3,
    verbose=1,
    n_jobs=-1
)

grid_search.fit(X_train_i, y_train_i)
print("Best Parameters:", grid_search.best_params_)

best_model = grid_search.best_estimator_
y_pred_best = best_model.predict(X_test_i)
evaluate_model(y_test_i, y_pred_best, name="Tuned XGB via GridSearch")

import shap
import matplotlib.pyplot as plt

# Use TreeExplainer (safe for XGBoost on CPU)
explainer = shap.TreeExplainer(best_model)
shap_values = explainer.shap_values(X_test_i)

# Summary Plot (bar or violin style)
shap.summary_plot(shap_values, X_test_i, plot_type="bar")  # use 'bar' for safety

mean_shap_vals = np.abs(shap_values).mean(axis=0)
shap_df = pd.DataFrame({
    "feature": X_test_i.columns,
    "mean_abs_shap": mean_shap_vals
}).sort_values(by="mean_abs_shap", ascending=False)

threshold = 0.00175
low_impact_features = shap_df[shap_df["mean_abs_shap"] < threshold]["feature"].tolist()
low_impact_features

X_shap_reduced = X.drop(columns=low_impact_features)

X_train_s, X_test_s, y_train_s, y_test_s = train_test_split(X_shap_reduced, y, test_size=0.2, random_state=42)
best_model.fit(X_train_s, y_train_s)
y_pred_s = best_model.predict(X_test_s)

evaluate_model(y_test_s, y_pred_s, name="XGBoost with SHAP-based Feature Selection")

residuals  = y_test_s - y_pred_s
sns.scatterplot(x=y_pred_s, y=residuals)

df[["popularity", "vote_count", "vote_ratio", "budget_to_popularity"]].hist(bins=50)

X["vote_popularity_product"] = X["vote_average"] * X["popularity"]
X["budget_vote_ratio"] = X["log_budget"] * X["vote_ratio"]

X_transformed = X.drop(columns=["popularity", "vote_count", "vote_ratio"])
X_train_log, X_test_log, y_train_log, y_test_log = train_test_split(X_transformed, y, test_size=0.2, random_state=42)
best_model.fit(X_train_log, y_train_log)
y_pred_log = best_model.predict(X_test_log)

evaluate_model(y_test_log, y_pred_log, name="XGBoost + Log-Transformed Features")

"""{'subsample': 0.8,
 'n_estimators': 300,
 'max_depth': 4,
 'learning_rate': 0.05,
 'gamma': 0,
 'colsample_bytree': 0.9}
"""

param_grid = {
    'n_estimators': [100, 300, 500],
    'max_depth': [4, 6, 8, 10],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 0.9, 1],
    'colsample_bytree': [0.7, 0.9, 1],
    'gamma': [0, 1, 5]
}

xgb_base = XGBRegressor(random_state=42)
random_search = RandomizedSearchCV(xgb_base, param_distributions=param_grid,
                                   n_iter=30, cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)

random_search.fit(X_train_log, y_train_log)

best_tuned_model = random_search.best_estimator_
y_pred_tuned = best_tuned_model.predict(X_test_log)
evaluate_model(y_test_log, y_pred_tuned, name="Tuned XGBoost Final")

from sklearn.metrics import r2_score

# Inputs
r2 = r2_score(y_test_log, y_pred_tuned)
n = X_test_log.shape[0]
p = X_test_log.shape[1]

# Adjusted R²
adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print(f"Adjusted R²: {adjusted_r2:.4f}")

import shap

explainer = shap.TreeExplainer(best_tuned_model)
shap_values = explainer.shap_values(X_test_log)

shap.summary_plot(shap_values, X_test_log)  # visual

# Top features numerically
shap_importance = np.abs(shap_values).mean(axis=0)
shap_series = pd.Series(shap_importance, index=X_test_log.columns).sort_values(ascending=False)

"""Keep top 30-40 SHAP Features Only"""

top_k = 35
top_features = shap_series.head(top_k).index.tolist()

X_train_reduced = X_train_log[top_features]
X_test_reduced = X_test_log[top_features]

best_tuned_model.fit(X_train_reduced, y_train_log)
y_pred_reduced = best_tuned_model.predict(X_test_reduced)

from sklearn.metrics import r2_score

r2 = r2_score(y_test_log, y_pred_reduced)
n = X_test_reduced.shape[0]
p = X_test_reduced.shape[1]

adjusted_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
print(f"R²: {r2:.4f} | Adjusted R²: {adjusted_r2:.4f}")

import joblib

joblib.dump(best_tuned_model, 'final_model.pkl')